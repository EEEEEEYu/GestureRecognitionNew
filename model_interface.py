import importlib
import inspect
from dataclasses import asdict

import torch
import lightning.pytorch as pl

from loss.loss_funcs import cross_entropy_loss
from torchmetrics.functional.classification import multiclass_accuracy
from configs.sections import (
    ModelConfig,
    OptimizerConfig,
    SchedulerConfig,
    TrainingConfig,
    DataConfig,
)


class ModelInterface(pl.LightningModule):
    def __init__(
        self,
        model_cfg: ModelConfig,
        optimizer_cfg: OptimizerConfig,
        scheduler_cfg: SchedulerConfig,
        training_cfg: TrainingConfig,
        data_cfg: DataConfig,
    ):
        super().__init__()
        self.model_cfg = model_cfg
        self.optimizer_cfg = optimizer_cfg
        self.scheduler_cfg = scheduler_cfg
        self.training_cfg = training_cfg
        self.data_cfg = data_cfg
        self.num_classes = self.data_cfg.dataset.dataset_init_args["num_classes"]

        self.save_hyperparameters(
            {
                "model": asdict(self.model_cfg),
                "optimizer": asdict(self.optimizer_cfg),
                "scheduler": asdict(self.scheduler_cfg),
                "training": asdict(self.training_cfg),
                "data": asdict(self.data_cfg),
            }
        )

        self.model = self.__load_model()
        self.loss_function = self.__configure_loss()

    def forward(self, x):
        return self.model(x)

    # For all these hook functions like on_XXX_<epoch|batch>_<end|start>(),
    # check document: https://lightning.ai/docs/pytorch/LTS/common/lightning_module.html
    # Epoch level training logging
    def on_train_epoch_end(self):
        pass

    # Caution: self.model.train() is invoked
    # For logging, check document: https://lightning.ai/docs/pytorch/stable/extensions/logging.html#automatic-logging
    # Important clarification for new users:
    # 1. If on_step=True, a _step suffix will be concatenated to metric name. Same for on_epoch, but epoch-level metrics will be automatically averaged using batch_size as weight.
    # 2. If enable_graph=True, .detach() will not be invoked on the value of metric. Could introduce potential error.
    # 3. If sync_dist=True, logger will average metrics across devices. This introduces additional communication overhead, and not suggested for large metric tensors.
    # We can also define customized metrics aggregator for incremental step-level aggregation(to be merged into epoch-level metrics).
    def training_step(self, batch, batch_idx):
        # DVSGesture batch format: dict with 'vectors', 'event_coords', 'labels'
        train_labels = batch['labels']
        train_out_logits = self(batch)
        
        # Get batch size from labels
        batch_size = train_labels.shape[0]
        
        # Compute loss with explicit batch_size
        train_loss = self.loss_function(train_out_logits, train_labels, 'train', batch_size)

        # Compute accuracy metrics
        train_step_top1_acc = multiclass_accuracy(train_out_logits, train_labels, num_classes=self.num_classes, average='micro', top_k=1)
        self.log('train_top1_acc', value=train_step_top1_acc, on_step=True, on_epoch=True, prog_bar=True, sync_dist=True, batch_size=batch_size)
        
        # Only compute top-3 for DVSGesture (11 classes)
        if self.num_classes >= 3:
            train_step_top3_acc = multiclass_accuracy(train_out_logits, train_labels, num_classes=self.num_classes, average='micro', top_k=3)
            self.log('train_top3_acc', value=train_step_top3_acc, on_step=True, on_epoch=True, prog_bar=True, sync_dist=True, batch_size=batch_size)

        train_step_output = {
            'loss': train_loss,
            'pred': train_out_logits,
            'ground_truth': train_labels
        }

        return train_step_output

    # Caution: self.model.eval() is invoked and this function executes within a <with torch.no_grad()> context
    def validation_step(self, batch, batch_idx):
        # DVSGesture batch format: dict with 'vectors', 'event_coords', 'labels'
        val_labels = batch['labels']
        val_out_logits = self(batch)
        
        # Get batch size from labels
        batch_size = val_labels.shape[0]
        
        # Compute loss with explicit batch_size
        val_loss = self.loss_function(val_out_logits, val_labels, 'val', batch_size)
        
        # Compute accuracy metrics
        val_step_top1_acc = multiclass_accuracy(val_out_logits, val_labels, num_classes=self.num_classes, average='micro', top_k=1)
        self.log('val_top1_acc', value=val_step_top1_acc, on_step=True, on_epoch=True, prog_bar=True, sync_dist=True, batch_size=batch_size)
        
        # Only compute top-3 for DVSGesture (11 classes)
        if self.num_classes >= 3:
            val_step_top3_acc = multiclass_accuracy(val_out_logits, val_labels, num_classes=self.num_classes, average='micro', top_k=3)
            self.log('val_top3_acc', value=val_step_top3_acc, on_step=True, on_epoch=True, prog_bar=True, sync_dist=True, batch_size=batch_size)

        val_step_output = {
            'loss': val_loss,
            'pred': val_out_logits,
            'ground_truth': val_labels
        }

        return val_step_output

    # Caution: self.model.eval() is invoked and this function executes within a <with torch.no_grad()> context
    def test_step(self, batch, batch_idx):
        # DVSGesture batch format: dict with 'vectors', 'event_coords', 'labels'
        test_labels = batch['labels']
        test_out_logits = self(batch)
        
        # Get batch size from labels
        batch_size = test_labels.shape[0]
        
        # Compute loss with explicit batch_size
        test_loss = self.loss_function(test_out_logits, test_labels, 'test', batch_size)
        
        # Compute accuracy metrics
        test_step_top1_acc = multiclass_accuracy(test_out_logits, test_labels, num_classes=self.num_classes, average='micro', top_k=1)
        self.log('test_top1_acc', value=test_step_top1_acc, on_step=True, on_epoch=True, prog_bar=True, sync_dist=True, batch_size=batch_size)
        
        # Only compute top-3 for DVSGesture (11 classes)
        if self.num_classes >= 3:
            test_step_top3_acc = multiclass_accuracy(test_out_logits, test_labels, num_classes=self.num_classes, average='micro', top_k=3)
            self.log('test_top3_acc', value=test_step_top3_acc, on_step=True, on_epoch=True, prog_bar=True, sync_dist=True, batch_size=batch_size)

        test_step_output = {
            'loss': test_loss,
            'pred': test_out_logits,
            'ground_truth': test_labels
        }

        return test_step_output

    def configure_optimizers(self):
        # https://docs.pytorch.org/docs/2.8/generated/torch.optim.Adam.html
        try:
            optimizer_class = getattr(torch.optim, self.optimizer_cfg.name)
        except AttributeError as exc:
            raise ValueError(f"Invalid optimizer: OPTIMIZER.{self.optimizer_cfg.name}") from exc

        # Split parameters into separate groups for weight decay
        # Reference: https://github.com/karpathy/minGPT/blob/master/mingpt/model.py
        decay = set()
        no_decay = set()
        whitelist_weight_modules = (torch.nn.Linear, torch.nn.Conv1d, torch.nn.Conv2d, torch.nn.Conv3d)
        blacklist_weight_modules = (torch.nn.LayerNorm, torch.nn.GroupNorm, torch.nn.BatchNorm1d, torch.nn.BatchNorm2d, torch.nn.BatchNorm3d, torch.nn.Embedding)
        
        for mn, m in self.model.named_modules():
            for pn, p in m.named_parameters():
                fpn = '%s.%s' % (mn, pn) if mn else pn # full param name
                
                if pn.endswith('bias'):
                    # all biases will not be decayed
                    no_decay.add(fpn)
                elif pn.endswith('weight') and isinstance(m, whitelist_weight_modules):
                    # weights of whitelist modules will be weight decayed
                    decay.add(fpn)
                elif pn.endswith('weight') and isinstance(m, blacklist_weight_modules):
                    # weights of blacklist modules will NOT be weight decayed
                    no_decay.add(fpn)
                
                # Special cases for Mamba parameters
                # A_log, D are specific to SSMs and usually shouldn't be decayed or have special rules
                # For simplicity here, we stick to standard rules: only standard weights get decay
                elif 'A_log' in pn or 'D' in pn:
                    no_decay.add(fpn)
        
        # Validate that we considered every parameter
        param_dict = {pn: p for pn, p in self.model.named_parameters()}
        inter_params = decay & no_decay
        union_params = decay | no_decay
        assert len(inter_params) == 0, "parameters %s made it into both decay/no_decay sets!" % (str(inter_params), )
        # Note: some parameters might not fall into the simple loops above (e.g. if they are directly in the module), 
        # so we add the rest to no_decay to be safe (e.g. learnable scalars)
        remaining_params = param_dict.keys() - union_params
        for p in remaining_params:
            no_decay.add(p)
            
        optim_groups = [
            {"params": [param_dict[pn] for pn in sorted(list(decay))], "weight_decay": self.optimizer_cfg.arguments.get('weight_decay', 0.0)},
            {"params": [param_dict[pn] for pn in sorted(list(no_decay))], "weight_decay": 0.0},
        ]

        optimizer_arguments = dict(self.optimizer_cfg.arguments or {})
        # Remove weight_decay from kwargs since it's handled in groups
        if 'weight_decay' in optimizer_arguments:
            del optimizer_arguments['weight_decay']
            
        optimizer_instance = optimizer_class(params=optim_groups, **optimizer_arguments)

        learning_rate_scheduler_cfg = self.scheduler_cfg.learning_rate
        if not learning_rate_scheduler_cfg.enabled:
            return [optimizer_instance]

        try:
            scheduler_class = getattr(torch.optim.lr_scheduler, learning_rate_scheduler_cfg.name)
        except AttributeError as exc:
            raise ValueError(
                f"Invalid learning rate scheduler: SCHEDULER.learning_rate.{learning_rate_scheduler_cfg.name}."
            ) from exc

        scheduler_arguments = dict(learning_rate_scheduler_cfg.arguments or {})
        scheduler_instance = scheduler_class(optimizer=optimizer_instance, **scheduler_arguments)

        return [optimizer_instance], [scheduler_instance]

    def __configure_loss(self):
        label_smoothing = self.training_cfg.label_smoothing
        
        def loss_func(preds, labels, stage, batch_size):
            # Only apply label smoothing during training
            smoothing = label_smoothing if stage == 'train' else 0.0
            
            CE_loss = 1.0 * cross_entropy_loss(pred=preds, gt=labels, label_smoothing=smoothing)
            self.log(f'{stage}_CE_loss', CE_loss, on_step=True, on_epoch=True, prog_bar=True, batch_size=batch_size)

            final_loss = CE_loss
            self.log(f'{stage}_loss', final_loss, on_step=True, on_epoch=True, prog_bar=True, batch_size=batch_size)

            return final_loss

        return loss_func
    
    @staticmethod
    def filter_init_args(cls, config_dict):
        """
        Checks if config_dict has all required arguments for cls.__init__
        """
        init_args = dict()
        for name in inspect.signature(cls.__init__).parameters.keys():
            # Skip 'self', '*args', '**kwargs' and parameters with defaults
            if name not in ('self'):
                init_args[name] = config_dict[name]
        provided_keys = set(config_dict.keys())
        missing_keys = init_args.keys() - provided_keys
        
        if missing_keys:
            raise ValueError(f"In dataset initialization, found missing config keys for {cls.__name__}: {missing_keys}")
        
        return init_args

    def __load_model(self):
        file_name = self.model_cfg.file_name
        class_name = self.model_cfg.class_name
        if class_name is None:
            raise ValueError("MODEL.class_name must be specified in the configuration.")
        if file_name is None:
            raise ValueError("MODEL.file_name must be specified in the configuration.")
        try:
            model_class = getattr(importlib.import_module('model.' + file_name, package=__package__), class_name)
        except Exception:
            raise ValueError(f'Invalid Module File Name or Invalid Class Name {file_name}.{class_name}!')

        model_init_kwargs = self.model_cfg.model_init_args
        # Only validate highest level keyword arguments. This is a tradeoff between flexibility and rigour.
        # If you want to enable recursive validation for every keyword including nested ones, define them as template
        # in config schema instead of using raw dictionary.
        # We assume that dataset_kwargs is a superset of data_class's init arg set.
        filtered_model_init_kwargs = self.filter_init_args(cls=model_class, config_dict=model_init_kwargs)
        model = model_class(**filtered_model_init_kwargs)
        if self.training_cfg.use_compile:
            model = torch.compile(model)
        return model
