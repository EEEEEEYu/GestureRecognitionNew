# Final Pre-Training Checklist

## âœ… Status: READY FOR TRAINING

All data pipeline components have been audited and validated. See `AUDIT_REPORT.md` for full details.

---

## Quick Summary

### âœ… What's Complete

1. **Preprocessing Pipeline**
   - âœ… Raw event loading (DVSGesture.py)
   - âœ… Vector encoding (SparseVKMEncoder.py)
   - âœ… Event coordinate storage
   - âœ… HDF5 compression and storage
   - âœ… Checkpointing and resume

2. **Dataloader**
   - âœ… Second-stage downsampling
   - âœ… Variable-length batching
   - âœ… Event coordinate loading
   - âœ… Perfect vector-coord alignment

3. **Configuration**
   - âœ… Two-stage ratio control
   - âœ… Consistent parameters
   - âœ… MODEL section updated for DVSGesture

4. **Helper Functions**
   - âœ… `create_dvsgesture_datasets()`
   - âœ… `create_dvsgesture_dataloaders()`

---

## Data Format Ready for Your Model

### Input Batch Structure

```python
batch = {
    'vectors': List[Tensor],           # [batch] each [num_vectors_i, 64] complex64
    'event_coords': List[ndarray],     # [batch] each [num_vectors_i, 4] float32
    'labels': Tensor,                  # [batch_size] int64
    'num_vectors_per_sample': List[int],
    'num_vectors_per_interval': List[List[int]],
    'file_paths': List[str],
    'num_intervals': List[int],
}
```

### Event Coordinates

```python
event_coords[i][j] = [x, y, t, p]
# x, y: spatial coordinates (0-127)
# t: timestamp in milliseconds
# p: polarity (0 or 1)
```

---

## Before Training: Run Validation

### Step 1: Run Preprocessing (if not done)

```bash
cd /fs/nexus-scratch/haowenyu/GestureRecognitionNew
./run_preprocess.sh
```

**Expected time**: 30-60 minutes  
**Expected output**: ~25 GB of HDF5 files

### Step 2: Validate Pipeline

```bash
mamba run -n torch python validate_pipeline.py
```

**Expected output**:
```
âœ… ALL VALIDATIONS PASSED!
ğŸš€ Your pipeline is READY FOR TRAINING!
```

---

## For Your Training Script

### Minimal Example

```python
import yaml
from data import create_dvsgesture_dataloaders

# Load config
with open('configs/config.yaml', 'r') as f:
    config = yaml.safe_load(f)

# Create dataloaders
loaders = create_dvsgesture_dataloaders(config)

# Get loaders
train_loader = loaders['train']      # ratio_of_vectors=0.8
val_loader = loaders['validation']    # ratio_of_vectors=1.0

# Training loop
for epoch in range(num_epochs):
    for batch in train_loader:
        # Extract data
        vectors = batch['vectors']          # List[Tensor[N_i, 64]]
        event_coords = batch['event_coords']  # List[Array[N_i, 4]]
        labels = batch['labels']            # Tensor[batch_size]
        
        # Process each sample
        logits_list = []
        for v, c in zip(vectors, event_coords):
            # Sort by time for sequential processing
            time_order = torch.argsort(torch.from_numpy(c[:, 2]))
            v_ordered = v[time_order]
            c_ordered = c[time_order]
            
            # Your model forward
            output = model(v_ordered, c_ordered)
            logits_list.append(output)
        
        logits = torch.stack(logits_list)
        
        # Compute loss
        loss = criterion(logits, labels)
        
        # Backprop
        optimizer.zero_grad()
        loss.backward()
        optimizer.step()
```

---

## Configuration Quick Reference

### First-Stage (Preprocessing)

```yaml
PRECOMPUTING:
  ratio_of_vectors: 0.1  # 10% of events â†’ vectors
  accumulation_interval_ms: 200.0
  encoding_dim: 64
```

### Second-Stage (Training)

```yaml
DATA:
  dataset:
    dataset_init_args:
      train_ratio_of_vectors: 0.8  # Use 80% during training
      val_ratio_of_vectors: 1.0    # Use 100% during validation
```

### Model Parameters

```yaml
MODEL:
  model_init_args:
    encoding_dim: 64  # From SparseVKMEncoder
    num_classes: 11   # DVSGesture classes
    # Add your architecture params
```

---

## File Structure

```
GestureRecognitionNew/
â”œâ”€â”€ configs/
â”‚   â””â”€â”€ config.yaml                    âœ… Updated and validated
â”œâ”€â”€ data/
â”‚   â”œâ”€â”€ __init__.py                    âœ… All exports ready
â”‚   â”œâ”€â”€ DVSGesture.py                  âœ… Raw event loader
â”‚   â”œâ”€â”€ SparseVKMEncoder.py            âœ… Encoder
â”‚   â”œâ”€â”€ DVSGesturePrecomputed.py       âœ… HDF5 dataloader
â”‚   â””â”€â”€ create_datasets.py             âœ… Helper functions
â”œâ”€â”€ preprocess_dvsgesture.py           âœ… Preprocessing script
â”œâ”€â”€ run_preprocess.sh                  âœ… Easy run script
â”œâ”€â”€ validate_pipeline.py               âœ… Validation script
â”œâ”€â”€ AUDIT_REPORT.md                    ğŸ“‹ Full audit details
â”œâ”€â”€ CONTROLLING_RATIOS.md              ğŸ“‹ Ratio configuration guide
â””â”€â”€ THIS_FILE.md                       ğŸ“‹ Final checklist
```

---

## Critical Points Verified âœ…

1. **Event-Vector Alignment**
   - Same query indices used for both
   - Order preserved through pipeline
   - Verified in preprocessing and dataloader

2. **Temporal Consistency**
   - `temporal_length = accumulation_interval_ms = 200.0`
   - Coordinates use same time reference

3. **Data Types**
   - Vectors: `torch.complex64` âœ…
   - Coordinates: `numpy.float32` âœ…
   - Labels: `torch.int64` âœ…

4. **Variable-Length Handling**
   - List-based batching âœ…
   - No padding required âœ…
   - Flexible for any model âœ…

5. **Configuration Consistency**
   - All height/width parameters match âœ…
   - Ratio controls in correct places âœ…
   - Model section updated âœ…

---

## Common Adjustments

### Change Augmentation Strength

Edit `configs/config.yaml`:
```yaml
train_ratio_of_vectors: 0.5  # More aggressive (50%)
train_ratio_of_vectors: 0.9  # More conservative (90%)
```

### Change Batch Size

Edit `configs/config.yaml`:
```yaml
DATA:
  dataloader:
    batch_size: 16        # Smaller for large models
    test_batch_size: 32   # Can be larger for validation
```

### Re-preprocess with Different Parameters

1. Update `PRECOMPUTING` section in config
2. Delete old output: `rm -rf precomputed_data/dvsgesture/`
3. Re-run: `./run_preprocess.sh`

---

## Troubleshooting

### Issue: "Precomputed file not found"
**Solution**: Run preprocessing first
```bash
./run_preprocess.sh
```

### Issue: "Shape mismatch"
**Solution**: Ensure config parameters match preprocessing
- Check `height`, `width`, `encoding_dim`

### Issue: "Out of memory"
**Solution**: Reduce batch size or ratio_of_vectors
```yaml
batch_size: 16  # Reduce from 32
```

---

## Performance Expectations

### Preprocessing
- **Time**: 30-60 minutes
- **Output**: ~25 GB
- **Resumable**: Yes (checkpoints every 50 samples)

### Training
- **Load speed**: 5-10x faster than raw events
- **Memory per batch**: ~670 MB (batch_size=32)
- **Epochs**: Depends on model complexity

---

## Final Steps

1. âœ… Review `AUDIT_REPORT.md` for full details
2. âš ï¸  Run `validate_pipeline.py` before training
3. âš ï¸  Implement your model architecture
4. âš ï¸  Update MODEL section with your model class
5. ğŸš€ Start training!

---

## Questions?

See documentation:
- **Full audit**: `AUDIT_REPORT.md`
- **Ratio control**: `CONTROLLING_RATIOS.md`
- **Model integration**: Section 6 of AUDIT_REPORT.md

---

**Status**: âœ… READY FOR TRAINING  
**Last Updated**: 2025-12-28  
**Pipeline Version**: v1.0
