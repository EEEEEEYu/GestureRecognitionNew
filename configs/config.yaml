TRAINING:
  deterministic: False
  use_compile: False
  inference_mode: False  # If this one is True, use inference
  seed: 42
  max_epochs: 100 # Increased from 50
  label_smoothing: 0.1 # Soft targets to prevent overconfidence

DISTRIBUTED:
  accelerator: gpu  # Changed from cpu - Mamba2 requires CUDA
  devices: 2
  num_nodes: 1
  strategy: auto

DATA:
  dataset:
    file_name: DVSGesturePrecomputed  # Changed from cifar10
    class_name: DVSGesturePrecomputed
    dataset_init_args:
      # DVSGesture-specific parameters
      precomputed_dir: /fs/nexus-scratch/haowenyu/GestureRecognitionNew/precomputed_data/dvsgesture
      height: 128
      width: 128
      num_classes: 11  # DVSGesture has 11 classes
      # Second-stage downsampling (training-time augmentation)
      train_ratio_of_vectors: 0.5  # Use 50% of the 30% precomputed vectors = 15% total (Changed from 0.8)
      val_ratio_of_vectors: 1.0    # Use 100% of precomputed vectors during validation
      use_flip_augmentation: False # Disabled as per request
      # Robustness Augmentations
      aug_jitter_std: 0.5       # Coordinate Jitter (pixels/time). Moderate strength.
      aug_drop_rate: 0.1        # Event Drop Rate (10%). Moderate strength.
      aug_time_scale_min: 0.8   # Temporal Scaling Min (20% faster).
      aug_time_scale_max: 1.2   # Temporal Scaling Max (20% slower).

  dataloader:
    batch_size: 8  # Reduced for memory efficiency with Mamba2
    test_batch_size: 16  # Reduced for validation
    num_workers: 4  # Reduced to avoid worker overhead
    persistent_workers: True
    pin_memory: True
    multiprocessing_context: fork
    drop_last: False
    shuffle_train: True
    shuffle_val: False
    shuffle_test: False

MODEL:
  file_name: sparse_hilbert_ssm
  class_name: SparseHilbertSSM
  model_init_args:
    # DVSGesture input parameters
    encoding_dim: 64  # From SparseVKMEncoder output
    num_classes: ${DATA.dataset.dataset_init_args.num_classes}
    input_meta:
      height: ${DATA.dataset.dataset_init_args.height}
      width: ${DATA.dataset.dataset_init_args.width}
      encoding_dim: 64  # Complex vector dimension
      num_classes: ${DATA.dataset.dataset_init_args.num_classes}
    # 3D SSM Architecture parameters
    hidden_dim: 256
    num_layers: 4  # Increased from 3 (deeper model)
    d_state: 128   # Increased from 64 (higher capacity for temporal dynamics)
    d_conv: 4      # Convolution kernel size in Mamba
    expand: 2      # Expansion factor in Mamba
    dropout: 0.2   # Dropout rate (reduced from 0.4 due to added DropPath)
    drop_path: 0.1 # Stochastic depth rate (DropPath)
    pooling_scales: [1, 2, 4]  # Multi-scale temporal pooling
    use_checkpoint: True  # Gradient checkpointing for memory efficiency

OPTIMIZER:
  name: AdamW
  arguments:
    lr: 1e-3
    weight_decay: 0.05  # Increased L2 Regularization (now excludes bias/norm)
  gradient_accumulation:
    enabled: False
    scheduling: {0: 4, 4: 2, 8: 1}
  gradient_clip:
    enabled: False
    gradient_clip_val: 1.0
    gradient_clip_algorithm: norm # norm or value
  stochastic_weight_averaging:
    enabled: False
    swa_lrs: 1e-2

SCHEDULER:
  learning_rate:      # For learning rate scheduler, check document: https://docs.pytorch.org/docs/2.8/optim.html#how-to-adjust-learning-rate
    enabled: True
    name: CosineAnnealingLR
    arguments:
      T_max: ${TRAINING.max_epochs}  # Matched to max_epochs
      eta_min: 1e-5
  early_stopping:
    enabled: False
    monitor: val_loss_epoch
    mode: min
    patience: 5
    min_delta: 1e-5
  
LOGGER:
  log_dir_root: lightning_logs
  experiment_name: simple_net_classify_v0

CHECKPOINT:
  enabled: True
  every_n_epochs: 1
  monitor: val_loss_epoch
  mode: min
  filename: "best-{epoch:03d}-{val_loss_epoch:.5f}"
  save_top_k: 1
  save_last: True

PRECOMPUTING:
  dataset_dir: /fs/nexus-projects/DVS_Actions/DVSGestureData
  output_dir: /fs/nexus-scratch/haowenyu/GestureRecognitionNew/precomputed_data/dvsgesture
  accumulation_interval_ms: 200.0  # Window size for splitting event stream
  ratio_of_vectors: 0.3  # Increased from 0.1 to allow better training-time sampling
  encoding_dim: 64
  temporal_length: 200.0  # Should match accumulation_interval_ms
  kernel_size: 17
  T_scale: 25.0
  S_scale: 25.0
  height: 128
  width: 128
  num_workers: 4  # Number of workers for parallel preprocessing
  checkpoint_every_n_samples: 50  # Save checkpoint every N samples during preprocessing
