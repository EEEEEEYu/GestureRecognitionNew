TRAINING:
  deterministic: False
  use_compile: False
  inference_mode: False  # If this one is True, use inference
  seed: 42
  max_epochs: 50

DISTRIBUTED:
  accelerator: gpu  # Changed from cpu - Mamba2 requires CUDA
  devices: 2
  num_nodes: 1
  strategy: auto

DATA:
  dataset:
    file_name: DVSGesturePrecomputed  # Changed from cifar10
    class_name: DVSGesturePrecomputed
    dataset_init_args:
      # DVSGesture-specific parameters
      precomputed_dir: /fs/nexus-scratch/haowenyu/GestureRecognitionNew/precomputed_data/dvsgesture
      height: 128
      width: 128
      num_classes: 11  # DVSGesture has 11 classes
      # Second-stage downsampling (training-time augmentation)
      train_ratio_of_vectors: 0.8  # Use 80% of precomputed vectors during training
      val_ratio_of_vectors: 1.0    # Use 100% (all) vectors during validation
      use_flip_augmentation: False
  dataloader:
    batch_size: 8  # Reduced for memory efficiency with Mamba2
    test_batch_size: 16  # Reduced for validation
    num_workers: 4  # Reduced to avoid worker overhead
    persistent_workers: True
    pin_memory: True
    multiprocessing_context: fork
    drop_last: False
    shuffle_train: True
    shuffle_val: False
    shuffle_test: False

MODEL:
  file_name: sparse_hilbert_ssm
  class_name: SparseHilbertSSM
  model_init_args:
    # DVSGesture input parameters
    encoding_dim: 64  # From SparseVKMEncoder output
    num_classes: ${DATA.dataset.dataset_init_args.num_classes}
    input_meta:
      height: ${DATA.dataset.dataset_init_args.height}
      width: ${DATA.dataset.dataset_init_args.width}
      encoding_dim: 64  # Complex vector dimension
      num_classes: ${DATA.dataset.dataset_init_args.num_classes}
    # 3D SSM Architecture parameters
    hidden_dim: 256
    num_layers: 3  # Number of Mamba2 layers per direction
    d_state: 64    # SSM state dimension
    d_conv: 4      # Convolution kernel size in Mamba
    expand: 2      # Expansion factor in Mamba
    dropout: 0.4   # Dropout rate (prevents overfitting)
    pooling_scales: [1, 2, 4]  # Multi-scale temporal pooling
    use_checkpoint: True  # Gradient checkpointing for memory efficiency

OPTIMIZER:
  name: AdamW
  arguments:
    lr: 1e-3
    weight_decay: 1e-3  # Default L2 Regularization
  gradient_accumulation:
    enabled: False
    scheduling: {0: 4, 4: 2, 8: 1}
  gradient_clip:
    enabled: False
    gradient_clip_val: 1.0
    gradient_clip_algorithm: norm # norm or value
  stochastic_weight_averaging:
    enabled: False
    swa_lrs: 1e-2

SCHEDULER:
  learning_rate:      # For learning rate scheduler, check document: https://docs.pytorch.org/docs/2.8/optim.html#how-to-adjust-learning-rate
    enabled: True
    name: CosineAnnealingLR
    arguments:
      T_max: 50  # Number of epochs by default
      eta_min: 1e-6
  early_stopping:
    enabled: False
    monitor: val_loss_epoch
    mode: min
    patience: 5
    min_delta: 1e-5
  
LOGGER:
  log_dir_root: lightning_logs
  experiment_name: simple_net_classify_v0

CHECKPOINT:
  enabled: True
  every_n_epochs: 1
  monitor: val_loss_epoch
  mode: min
  filename: "best-{epoch:03d}-{val_loss_epoch:.5f}"
  save_top_k: 1
  save_last: True

PRECOMPUTING:
  dataset_dir: /fs/nexus-projects/DVS_Actions/DVSGestureData
  output_dir: /fs/nexus-scratch/haowenyu/GestureRecognitionNew/precomputed_data/dvsgesture
  accumulation_interval_ms: 200.0  # Window size for splitting event stream
  ratio_of_vectors: 0.1  # First stage downsampling: ratio of vectors to events for precomputation
  encoding_dim: 64
  temporal_length: 200.0  # Should match accumulation_interval_ms
  kernel_size: 17
  T_scale: 25.0
  S_scale: 25.0
  height: 128
  width: 128
  num_workers: 4  # Number of workers for parallel preprocessing
  checkpoint_every_n_samples: 50  # Save checkpoint every N samples during preprocessing
